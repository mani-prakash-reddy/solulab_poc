{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9133c734",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-09 21:43:14.978483: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Train: 541462, Val: 29196, Test: 29196\n",
      "Train: 541462, Val: 29196, Test: 29196\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "label_cols = [\n",
    "    \"toxicity\",\n",
    "    \"severe_toxicity\",\n",
    "    \"obscene\",\n",
    "    \"threat\",\n",
    "    \"insult\",\n",
    "    \"identity_attack\",\n",
    "    \"sexual_explicit\",\n",
    "]\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 2\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Dataset\n",
    "ds = load_dataset(\"google/civil_comments\")\n",
    "train_ds = ds[\"train\"]\n",
    "val_ds = ds[\"validation\"]\n",
    "test_ds = ds.get(\"test\")\n",
    "\n",
    "# 30% of the data for quicker runs\n",
    "train_ds = train_ds.shuffle(seed=42).select(range(int(0.3 * len(train_ds))))\n",
    "val_ds = val_ds.shuffle(seed=42).select(range(int(0.3 * len(val_ds))))\n",
    "if test_ds:\n",
    "    test_ds = test_ds.shuffle(seed=42).select(range(int(0.3 * len(test_ds))))\n",
    "\n",
    "print(\n",
    "    f\"Train: {len(train_ds)}, Val: {len(val_ds)}, Test: {len(test_ds) if test_ds else 0}\"\n",
    ")\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8677a1ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0867bdac75c4150b6b08bcc15b77761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/541462 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c582cbb653e7420e9bab06061d0bee02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/29196 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c28c571eac8143af9543b91ba1dcf2bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/29196 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess(batch):\n",
    "    enc = tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        max_length=MAX_LEN,\n",
    "    )\n",
    "    labels = []\n",
    "    for i in range(len(batch[\"text\"])):\n",
    "        labels.append([1.0 if float(batch[c][i]) >= 0.5 else 0.0 for c in label_cols])\n",
    "    enc[\"labels\"] = labels\n",
    "    return enc\n",
    "\n",
    "\n",
    "train_enc = train_ds.map(preprocess, batched=True, remove_columns=train_ds.column_names)\n",
    "val_enc = val_ds.map(preprocess, batched=True, remove_columns=val_ds.column_names)\n",
    "\n",
    "test_enc = None\n",
    "if test_ds is not None:\n",
    "    test_enc = test_ds.map(\n",
    "        preprocess, batched=True, remove_columns=test_ds.column_names\n",
    "    )\n",
    "\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7268004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d42a2fbdf18042d3815a321f3c2b9154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_labels = len(label_cols)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=num_labels,\n",
    "    problem_type=\"multi_label_classification\",\n",
    ").to(device)\n",
    "\n",
    "id2label = {i: name for i, name in enumerate(label_cols)}\n",
    "label2id = {name: i for i, name in enumerate(label_cols)}\n",
    "model.config.id2label = id2label\n",
    "model.config.label2id = label2id\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = 1 / (1 + np.exp(-logits))\n",
    "    preds = (probs >= 0.5).astype(int)\n",
    "\n",
    "    rows = {}\n",
    "    f1_macro = f1_score(labels, preds, average=\"macro\", zero_division=0)\n",
    "    exact_acc = (preds == labels).all(axis=1).mean()\n",
    "\n",
    "    # per-label metrics\n",
    "    per_label = {}\n",
    "    for i, name in enumerate(label_cols):\n",
    "        y = labels[:, i]\n",
    "        p = preds[:, i]\n",
    "        s = probs[:, i]\n",
    "        try:\n",
    "            auc = roc_auc_score(y, s) if len(np.unique(y)) > 1 else np.nan\n",
    "        except Exception:\n",
    "            auc = np.nan\n",
    "        per_label[name] = {\n",
    "            \"auc\": auc,\n",
    "            \"f1\": f1_score(y, p, zero_division=0),\n",
    "            \"acc\": accuracy_score(y, p),\n",
    "        }\n",
    "\n",
    "    rows[\"f1_macro\"] = f1_macro\n",
    "    rows[\"exact_acc\"] = exact_acc\n",
    "    # flatten per-label with prefixes\n",
    "    for k, v in per_label.items():\n",
    "        rows[f\"{k}_auc\"] = v[\"auc\"]\n",
    "        rows[f\"{k}_f1\"] = v[\"f1\"]\n",
    "        rows[f\"{k}_acc\"] = v[\"acc\"]\n",
    "\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cfe6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mani/.pyenv/lib/python3.12/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_56100/3367888698.py:17: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "347a1dacb77c44e7b0b025c591d043c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/67684 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3042, 'grad_norm': 0.5533372163772583, 'learning_rate': 1.9985225459488213e-05, 'epoch': 0.0}\n",
      "{'loss': 0.1158, 'grad_norm': 0.227631613612175, 'learning_rate': 1.997045091897642e-05, 'epoch': 0.0}\n",
      "{'loss': 0.1158, 'grad_norm': 0.227631613612175, 'learning_rate': 1.997045091897642e-05, 'epoch': 0.0}\n",
      "{'loss': 0.0958, 'grad_norm': 0.3895963728427887, 'learning_rate': 1.995567637846463e-05, 'epoch': 0.0}\n",
      "{'loss': 0.0958, 'grad_norm': 0.3895963728427887, 'learning_rate': 1.995567637846463e-05, 'epoch': 0.0}\n",
      "{'loss': 0.0867, 'grad_norm': 0.13505657017230988, 'learning_rate': 1.9940901837952842e-05, 'epoch': 0.01}\n",
      "{'loss': 0.0867, 'grad_norm': 0.13505657017230988, 'learning_rate': 1.9940901837952842e-05, 'epoch': 0.01}\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_ckpt\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE * 2,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    greater_is_better=True,\n",
    "    logging_steps=50,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_enc,\n",
    "    eval_dataset=val_enc,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3a0e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_raw = trainer.predict(val_enc)\n",
    "val_logits = val_raw.predictions\n",
    "val_labels = val_raw.label_ids\n",
    "val_probs = 1 / (1 + np.exp(-val_logits))\n",
    "val_pred = (val_probs >= 0.5).astype(int)\n",
    "\n",
    "if test_enc is not None:\n",
    "    test_raw = trainer.predict(test_enc)\n",
    "    test_logits = test_raw.predictions\n",
    "    test_labels = test_raw.label_ids\n",
    "    test_probs = 1 / (1 + np.exp(-test_logits))\n",
    "    test_pred = (test_probs >= 0.5).astype(int)\n",
    "else:\n",
    "    test_logits = test_labels = test_probs = test_pred = None\n",
    "\n",
    "rows = []\n",
    "for i, label in enumerate(label_cols):\n",
    "    vy = val_labels[:, i]\n",
    "    vs = val_probs[:, i]\n",
    "    vp = val_pred[:, i]\n",
    "    v_auc = roc_auc_score(vy, vs) if len(np.unique(vy)) > 1 else np.nan\n",
    "    v_f1 = f1_score(vy, vp, zero_division=0)\n",
    "    v_acc = accuracy_score(vy, vp)\n",
    "\n",
    "    row = {\"label\": label, \"val_auc\": v_auc, \"val_f1\": v_f1, \"val_acc\": v_acc}\n",
    "    if test_probs is not None:\n",
    "        ty = test_labels[:, i]\n",
    "        ts = test_probs[:, i]\n",
    "        tp = test_pred[:, i]\n",
    "        t_auc = roc_auc_score(ty, ts) if len(np.unique(ty)) > 1 else np.nan\n",
    "        t_f1 = f1_score(ty, tp, zero_division=0)\n",
    "        t_acc = accuracy_score(ty, tp)\n",
    "        row.update({\"test_auc\": t_auc, \"test_f1\": t_f1, \"test_acc\": t_acc})\n",
    "    else:\n",
    "        row.update({\"test_auc\": np.nan, \"test_f1\": np.nan, \"test_acc\": np.nan})\n",
    "    rows.append(row)\n",
    "\n",
    "metrics_df = pd.DataFrame(\n",
    "    rows,\n",
    "    columns=[\n",
    "        \"label\",\n",
    "        \"val_auc\",\n",
    "        \"val_f1\",\n",
    "        \"val_acc\",\n",
    "        \"test_auc\",\n",
    "        \"test_f1\",\n",
    "        \"test_acc\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "avg_vals = metrics_df.drop(columns=[\"label\"]).mean(numeric_only=True)\n",
    "avg_row = {**{\"label\": \"AVG\"}, **avg_vals.to_dict()}\n",
    "metrics_df = pd.concat([metrics_df, pd.DataFrame([avg_row])], ignore_index=True)\n",
    "metrics_df = metrics_df.round(3)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f7b0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU inference benchmarking\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "model_cpu = model.to(\"cpu\").eval()\n",
    "\n",
    "bench_ds = test_enc if test_enc is not None else val_enc\n",
    "bench_loader = DataLoader(\n",
    "    bench_ds,\n",
    "    batch_size=BATCH_SIZE * 2,\n",
    "    shuffle=False,\n",
    "    collate_fn=collator,\n",
    ")\n",
    "\n",
    "# Warm-up\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(bench_loader):\n",
    "        inputs = {\n",
    "            k: v.to(\"cpu\")\n",
    "            for k, v in batch.items()\n",
    "            if k in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]\n",
    "        }\n",
    "        _ = model_cpu(**inputs)\n",
    "        break\n",
    "\n",
    "# Timed pass\n",
    "n_samples = 0\n",
    "start = time.perf_counter()\n",
    "with torch.no_grad():\n",
    "    for batch in bench_loader:\n",
    "        inputs = {\n",
    "            k: v.to(\"cpu\")\n",
    "            for k, v in batch.items()\n",
    "            if k in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]\n",
    "        }\n",
    "        _ = model_cpu(**inputs)\n",
    "        n_samples += inputs[\"input_ids\"].size(0)\n",
    "end = time.perf_counter()\n",
    "\n",
    "total_seconds = end - start\n",
    "throughput = n_samples / total_seconds if total_seconds > 0 else float(\"inf\")\n",
    "per_sample_ms_all = (total_seconds / n_samples) * 1000.0\n",
    "\n",
    "per_label_timings = [\n",
    "    {\n",
    "        \"label\": label,\n",
    "        \"test_infer_seconds\": total_seconds,\n",
    "        \"per_sample_ms\": per_sample_ms_all,\n",
    "    }\n",
    "    for label in label_cols\n",
    "]\n",
    "\n",
    "time_metrics_df = pd.DataFrame(per_label_timings)\n",
    "\n",
    "# Append AVG row\n",
    "avg_vals = time_metrics_df.drop(columns=[\"label\"]).mean(numeric_only=True)\n",
    "avg_row = {**{\"label\": \"AVG\"}, **avg_vals.to_dict()}\n",
    "time_metrics_df = pd.concat(\n",
    "    [time_metrics_df, pd.DataFrame([avg_row])], ignore_index=True\n",
    ")\n",
    "\n",
    "# Round for readability\n",
    "time_metrics_df[\"test_infer_seconds\"] = time_metrics_df[\"test_infer_seconds\"].round(6)\n",
    "time_metrics_df[\"per_sample_ms\"] = time_metrics_df[\"per_sample_ms\"].round(6)\n",
    "\n",
    "print(\"Benchmark (CPU) on test set:\")\n",
    "print(f\" - samples: {n_samples}\")\n",
    "print(f\" - total_inference_seconds_all_labels: {total_seconds:.6f}\")\n",
    "print(f\" - throughput_samples_per_sec_all_labels: {throughput:.2f}\")\n",
    "print(f\" - avg_per_sample_latency_ms_all_labels: {per_sample_ms_all:.6f}\")\n",
    "\n",
    "time_metrics_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".pyenv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
