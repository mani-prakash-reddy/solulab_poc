{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e77aa73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"google/civil_comments\")\n",
    "label_cols = [\n",
    "    \"toxicity\",\n",
    "    \"severe_toxicity\",\n",
    "    \"obscene\",\n",
    "    \"threat\",\n",
    "    \"insult\",\n",
    "    \"identity_attack\",\n",
    "    \"sexual_explicit\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcc05e2c",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "splits = list(ds.keys())\n",
    "train_ds = ds[\"train\"] if \"train\" in ds else ds[splits[0]]\n",
    "val_ds = (\n",
    "    ds[\"validation\"] if \"validation\" in ds else (ds[\"test\"] if \"test\" in ds else None)\n",
    ")\n",
    "test_ds = ds[\"test\"] if (\"test\" in ds and val_ds is not ds.get(\"test\")) else None\n",
    "\n",
    "# Prepare texts for vectorization\n",
    "texts_train = np.array(train_ds[\"text\"])\n",
    "if val_ds is None:\n",
    "    base_strat = (np.array(train_ds[label_cols[0]]) >= 0.5).astype(int)\n",
    "    idx_train, idx_val = train_test_split(\n",
    "        np.arange(len(texts_train)), test_size=0.2, random_state=42, stratify=base_strat\n",
    "    )\n",
    "    tr_texts = texts_train[idx_train]\n",
    "    val_texts = texts_train[idx_val]\n",
    "    use_split_indices = (idx_train, idx_val)\n",
    "else:\n",
    "    tr_texts = texts_train\n",
    "    val_texts = np.array(val_ds[\"text\"])\n",
    "    use_split_indices = None\n",
    "\n",
    "test_texts = np.array(test_ds[\"text\"]) if test_ds is not None else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1df084bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized: train=(1804874, 100000), val=(97320, 100000), test=(97320, 100000)\n",
      "toxicity: val_auc=0.947 val_f1=0.585 val_acc=0.908 | test_auc=0.947 test_f1=0.591 test_acc=0.908\n",
      "toxicity: val_auc=0.947 val_f1=0.585 val_acc=0.908 | test_auc=0.947 test_f1=0.591 test_acc=0.908\n",
      "severe_toxicity: val_auc=nan val_f1=0.000 val_acc=1.000 | test_auc=nan test_f1=0.000 test_acc=1.000\n",
      "severe_toxicity: val_auc=nan val_f1=0.000 val_acc=1.000 | test_auc=nan test_f1=0.000 test_acc=1.000\n",
      "obscene: val_auc=0.960 val_f1=0.385 val_acc=0.986 | test_auc=0.971 test_f1=0.411 test_acc=0.987\n",
      "obscene: val_auc=0.960 val_f1=0.385 val_acc=0.986 | test_auc=0.971 test_f1=0.411 test_acc=0.987\n",
      "threat: val_auc=0.962 val_f1=0.174 val_acc=0.985 | test_auc=0.955 test_f1=0.151 test_acc=0.984\n",
      "threat: val_auc=0.962 val_f1=0.174 val_acc=0.985 | test_auc=0.955 test_f1=0.151 test_acc=0.984\n",
      "insult: val_auc=0.960 val_f1=0.584 val_acc=0.928 | test_auc=0.959 test_f1=0.585 test_acc=0.928\n",
      "insult: val_auc=0.960 val_f1=0.584 val_acc=0.928 | test_auc=0.959 test_f1=0.585 test_acc=0.928\n",
      "identity_attack: val_auc=0.967 val_f1=0.272 val_acc=0.971 | test_auc=0.969 test_f1=0.278 test_acc=0.971\n",
      "identity_attack: val_auc=0.967 val_f1=0.272 val_acc=0.971 | test_auc=0.969 test_f1=0.278 test_acc=0.971\n",
      "sexual_explicit: val_auc=0.963 val_f1=0.270 val_acc=0.991 | test_auc=0.970 test_f1=0.316 test_acc=0.992\n",
      "sexual_explicit: val_auc=0.963 val_f1=0.270 val_acc=0.991 | test_auc=0.970 test_f1=0.316 test_acc=0.992\n"
     ]
    }
   ],
   "source": [
    "# Vectorize text once and reuse for all labels\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=100_000,\n",
    "    min_df=2,\n",
    "    stop_words=\"english\",\n",
    "    dtype=np.float32,\n",
    ")\n",
    "X_train = tfidf_vectorizer.fit_transform(tr_texts)\n",
    "X_val = tfidf_vectorizer.transform(val_texts)\n",
    "X_test = tfidf_vectorizer.transform(test_texts) if test_texts is not None else None\n",
    "\n",
    "\n",
    "def safe_auc(y_true, y_score):\n",
    "    y_true = np.asarray(y_true)\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        return float(\"nan\")\n",
    "    return roc_auc_score(y_true, y_score)\n",
    "\n",
    "\n",
    "# Train one model per label\n",
    "logreg_models = {}\n",
    "metrics_rows = []\n",
    "print(\n",
    "    f\"Vectorized: train={X_train.shape}, val={X_val.shape}\"\n",
    "    + (f\", test={X_test.shape}\" if X_test is not None else \"\")\n",
    ")\n",
    "\n",
    "for label in label_cols:\n",
    "    # Building binary targets (threshold at 0.5)\n",
    "    y_full_train = np.array(train_ds[label])\n",
    "    if use_split_indices is None:\n",
    "        y_train = (y_full_train >= 0.5).astype(int)\n",
    "        y_val = (np.array(val_ds[label]) >= 0.5).astype(int)\n",
    "    else:\n",
    "        idx_tr, idx_v = use_split_indices\n",
    "        y_train = (y_full_train[idx_tr] >= 0.5).astype(int)\n",
    "        y_val = (y_full_train[idx_v] >= 0.5).astype(int)\n",
    "\n",
    "    y_test = (\n",
    "        (np.array(test_ds[label]) >= 0.5).astype(int) if X_test is not None else None\n",
    "    )\n",
    "\n",
    "    # Logistic Regression model\n",
    "    clf = LogisticRegression(\n",
    "        solver=\"liblinear\",\n",
    "        class_weight=\"balanced\",\n",
    "        max_iter=1000,\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "    logreg_models[label] = clf\n",
    "\n",
    "    # Validation metrics\n",
    "    val_scores = clf.decision_function(X_val)\n",
    "    val_pred = (val_scores >= 0).astype(int)\n",
    "    val_auc = safe_auc(y_val, val_scores)\n",
    "    val_f1 = f1_score(y_val, val_pred)\n",
    "    val_acc = accuracy_score(y_val, val_pred)\n",
    "\n",
    "    msg = f\"{label}: val_auc={val_auc:.3f} val_f1={val_f1:.3f} val_acc={val_acc:.3f}\"\n",
    "\n",
    "    row = {\n",
    "        \"label\": label,\n",
    "        \"val_auc\": float(val_auc) if not np.isnan(val_auc) else np.nan,\n",
    "        \"val_f1\": float(val_f1),\n",
    "        \"val_acc\": float(val_acc),\n",
    "    }\n",
    "\n",
    "    # Test metrics\n",
    "    if X_test is not None:\n",
    "        test_scores = clf.decision_function(X_test)\n",
    "        test_pred = (test_scores >= 0).astype(int)\n",
    "        test_auc = safe_auc(y_test, test_scores)\n",
    "        test_f1 = f1_score(y_test, test_pred)\n",
    "        test_acc = accuracy_score(y_test, test_pred)\n",
    "        msg += (\n",
    "            f\" | test_auc={test_auc:.3f} test_f1={test_f1:.3f} test_acc={test_acc:.3f}\"\n",
    "        )\n",
    "\n",
    "        row.update(\n",
    "            {\n",
    "                \"test_auc\": float(test_auc) if not np.isnan(test_auc) else np.nan,\n",
    "                \"test_f1\": float(test_f1),\n",
    "                \"test_acc\": float(test_acc),\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        row.update(\n",
    "            {\n",
    "                \"test_auc\": np.nan,\n",
    "                \"test_f1\": np.nan,\n",
    "                \"test_acc\": np.nan,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    metrics_rows.append(row)\n",
    "    print(msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6413a8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "label",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "val_auc",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "val_f1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "val_acc",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "test_auc",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "test_f1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "test_acc",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "cee59930-e0b5-4e0d-bc1f-2e37b760b795",
       "rows": [
        [
         "0",
         "toxicity",
         "0.947",
         "0.585",
         "0.908",
         "0.947",
         "0.591",
         "0.908"
        ],
        [
         "1",
         "severe_toxicity",
         null,
         "0.0",
         "1.0",
         null,
         "0.0",
         "1.0"
        ],
        [
         "2",
         "obscene",
         "0.96",
         "0.385",
         "0.986",
         "0.971",
         "0.411",
         "0.987"
        ],
        [
         "3",
         "threat",
         "0.962",
         "0.174",
         "0.985",
         "0.955",
         "0.151",
         "0.984"
        ],
        [
         "4",
         "insult",
         "0.96",
         "0.584",
         "0.928",
         "0.959",
         "0.585",
         "0.928"
        ],
        [
         "5",
         "identity_attack",
         "0.967",
         "0.272",
         "0.971",
         "0.969",
         "0.278",
         "0.971"
        ],
        [
         "6",
         "sexual_explicit",
         "0.963",
         "0.27",
         "0.991",
         "0.97",
         "0.316",
         "0.992"
        ],
        [
         "7",
         "AVG",
         "0.96",
         "0.325",
         "0.967",
         "0.962",
         "0.333",
         "0.967"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 8
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>val_auc</th>\n",
       "      <th>val_f1</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>test_auc</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>test_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>toxicity</td>\n",
       "      <td>0.947</td>\n",
       "      <td>0.585</td>\n",
       "      <td>0.908</td>\n",
       "      <td>0.947</td>\n",
       "      <td>0.591</td>\n",
       "      <td>0.908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>severe_toxicity</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>obscene</td>\n",
       "      <td>0.960</td>\n",
       "      <td>0.385</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.971</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>threat</td>\n",
       "      <td>0.962</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.985</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>insult</td>\n",
       "      <td>0.960</td>\n",
       "      <td>0.584</td>\n",
       "      <td>0.928</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.585</td>\n",
       "      <td>0.928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>identity_attack</td>\n",
       "      <td>0.967</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.971</td>\n",
       "      <td>0.969</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sexual_explicit</td>\n",
       "      <td>0.963</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AVG</td>\n",
       "      <td>0.960</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.967</td>\n",
       "      <td>0.962</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.967</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             label  val_auc  val_f1  val_acc  test_auc  test_f1  test_acc\n",
       "0         toxicity    0.947   0.585    0.908     0.947    0.591     0.908\n",
       "1  severe_toxicity      NaN   0.000    1.000       NaN    0.000     1.000\n",
       "2          obscene    0.960   0.385    0.986     0.971    0.411     0.987\n",
       "3           threat    0.962   0.174    0.985     0.955    0.151     0.984\n",
       "4           insult    0.960   0.584    0.928     0.959    0.585     0.928\n",
       "5  identity_attack    0.967   0.272    0.971     0.969    0.278     0.971\n",
       "6  sexual_explicit    0.963   0.270    0.991     0.970    0.316     0.992\n",
       "7              AVG    0.960   0.325    0.967     0.962    0.333     0.967"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build metrics table\n",
    "metrics_df = pd.DataFrame(\n",
    "    metrics_rows,\n",
    "    columns=[\n",
    "        \"label\",\n",
    "        \"val_auc\",\n",
    "        \"val_f1\",\n",
    "        \"val_acc\",\n",
    "        \"test_auc\",\n",
    "        \"test_f1\",\n",
    "        \"test_acc\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Compute mean of numeric columns\n",
    "avg_vals = metrics_df.drop(columns=[\"label\"]).mean(numeric_only=True)\n",
    "avg_row = {**{\"label\": \"AVG\"}, **avg_vals.to_dict()}\n",
    "metrics_df = pd.concat([metrics_df, pd.DataFrame([avg_row])], ignore_index=True)\n",
    "\n",
    "# Round for readability\n",
    "for col in metrics_df.columns:\n",
    "    if col != \"label\":\n",
    "        metrics_df[col] = metrics_df[col].round(3)\n",
    "\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6391d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "# Create a directory to save the models if it doesn't exist\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Save the TF-IDF vectorizer\n",
    "joblib.dump(tfidf_vectorizer, 'models/tfidf_vectorizer.joblib')\n",
    "\n",
    "# Save each logistic regression model\n",
    "for label, model in logreg_models.items():\n",
    "    model_filename = f'models/logreg_{label}.joblib'\n",
    "    joblib.dump(model, model_filename)\n",
    "\n",
    "print(\"Models and vectorizer saved in the 'models' directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e4f0d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark (CPU) on test set:\n",
      " - samples: 97320\n",
      " - total_inference_seconds_all_labels: 0.075445\n",
      " - throughput_samples_per_sec_all_labels: 1289947.60\n",
      " - avg_per_sample_latency_ms_all_labels: 0.001\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "label",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "test_infer_seconds",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "per_sample_ms",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "188d7919-325c-4d25-91a0-93797761b414",
       "rows": [
        [
         "0",
         "toxicity",
         "0.011311",
         "0.0"
        ],
        [
         "1",
         "severe_toxicity",
         "0.011469",
         "0.0"
        ],
        [
         "2",
         "obscene",
         "0.009411",
         "0.0"
        ],
        [
         "3",
         "threat",
         "0.008802",
         "0.0"
        ],
        [
         "4",
         "insult",
         "0.010641",
         "0.0"
        ],
        [
         "5",
         "identity_attack",
         "0.010876",
         "0.0"
        ],
        [
         "6",
         "sexual_explicit",
         "0.012666",
         "0.0"
        ],
        [
         "7",
         "AVG",
         "0.01074",
         "0.0"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 8
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>test_infer_seconds</th>\n",
       "      <th>per_sample_ms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>toxicity</td>\n",
       "      <td>0.011311</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>severe_toxicity</td>\n",
       "      <td>0.011469</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>obscene</td>\n",
       "      <td>0.009411</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>threat</td>\n",
       "      <td>0.008802</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>insult</td>\n",
       "      <td>0.010641</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>identity_attack</td>\n",
       "      <td>0.010876</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sexual_explicit</td>\n",
       "      <td>0.012666</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AVG</td>\n",
       "      <td>0.010740</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             label  test_infer_seconds  per_sample_ms\n",
       "0         toxicity            0.011311            0.0\n",
       "1  severe_toxicity            0.011469            0.0\n",
       "2          obscene            0.009411            0.0\n",
       "3           threat            0.008802            0.0\n",
       "4           insult            0.010641            0.0\n",
       "5  identity_attack            0.010876            0.0\n",
       "6  sexual_explicit            0.012666            0.0\n",
       "7              AVG            0.010740            0.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CPU inference benchmarking on test set (LogReg one-vs-rest)\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Optional warm-up to avoid one-time overheads\n",
    "_ = logreg_models[label_cols[0]].decision_function(X_test)\n",
    "\n",
    "per_label_timings = []\n",
    "t_total_start = time.perf_counter()\n",
    "for label in label_cols:\n",
    "    clf = logreg_models[label]\n",
    "    t0 = time.perf_counter()\n",
    "    _ = clf.decision_function(X_test)\n",
    "    t1 = time.perf_counter()\n",
    "    per_label_timings.append({\n",
    "        \"label\": label,\n",
    "        \"test_infer_seconds\": t1 - t0,\n",
    "    })\n",
    "t_total_end = time.perf_counter()\n",
    "\n",
    "total_seconds = t_total_end - t_total_start\n",
    "n_samples = X_test.shape[0]\n",
    "\n",
    "# Build timing table\n",
    "time_metrics_df = pd.DataFrame(per_label_timings)\n",
    "time_metrics_df[\"per_sample_ms\"] = (time_metrics_df[\"test_infer_seconds\"] / n_samples) * 1000.0\n",
    "\n",
    "# Append AVG row\n",
    "avg_vals = time_metrics_df.drop(columns=[\"label\"]).mean(numeric_only=True)\n",
    "avg_row = {**{\"label\": \"AVG\"}, **avg_vals.to_dict()}\n",
    "time_metrics_df = pd.concat([time_metrics_df, pd.DataFrame([avg_row])], ignore_index=True)\n",
    "\n",
    "# Round for readability\n",
    "time_metrics_df[\"test_infer_seconds\"] = time_metrics_df[\"test_infer_seconds\"].round(6)\n",
    "time_metrics_df[\"per_sample_ms\"] = time_metrics_df[\"per_sample_ms\"].round(3)\n",
    "\n",
    "# Summary\n",
    "throughput = n_samples / total_seconds if total_seconds > 0 else float(\"inf\")\n",
    "per_sample_ms_all = (total_seconds / n_samples) * 1000.0\n",
    "\n",
    "print(\"Benchmark (CPU) on test set:\")\n",
    "print(f\" - samples: {n_samples}\")\n",
    "print(f\" - total_inference_seconds_all_labels: {total_seconds:.6f}\")\n",
    "print(f\" - throughput_samples_per_sec_all_labels: {throughput:.2f}\")\n",
    "print(f\" - avg_per_sample_latency_ms_all_labels: {per_sample_ms_all:.3f}\")\n",
    "\n",
    "time_metrics_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
