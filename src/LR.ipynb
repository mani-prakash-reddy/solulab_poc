{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e77aa73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"google/civil_comments\")\n",
    "label_cols = [\n",
    "    \"toxicity\",\n",
    "    \"severe_toxicity\",\n",
    "    \"obscene\",\n",
    "    \"threat\",\n",
    "    \"insult\",\n",
    "    \"identity_attack\",\n",
    "    \"sexual_explicit\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcc05e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "splits = list(ds.keys())\n",
    "train_ds = ds[\"train\"] if \"train\" in ds else ds[splits[0]]\n",
    "val_ds = (\n",
    "    ds[\"validation\"] if \"validation\" in ds else (ds[\"test\"] if \"test\" in ds else None)\n",
    ")\n",
    "test_ds = ds[\"test\"] if (\"test\" in ds and val_ds is not ds.get(\"test\")) else None\n",
    "\n",
    "# # only 10k samples for faster experimentation\n",
    "# train_ds = train_ds.select(range(100000))\n",
    "# if val_ds is not None:\n",
    "#     val_ds = val_ds.select(range(1000))\n",
    "# if test_ds is not None:\n",
    "#     test_ds = test_ds.select(range(1000))\n",
    "\n",
    "# Prepare texts for vectorization\n",
    "texts_train = np.array(train_ds[\"text\"])\n",
    "if val_ds is None:\n",
    "    base_strat = (np.array(train_ds[label_cols[0]]) >= 0.5).astype(int)\n",
    "    idx_train, idx_val = train_test_split(\n",
    "        np.arange(len(texts_train)), test_size=0.2, random_state=42, stratify=base_strat\n",
    "    )\n",
    "    tr_texts = texts_train[idx_train]\n",
    "    val_texts = texts_train[idx_val]\n",
    "    use_split_indices = (idx_train, idx_val)\n",
    "else:\n",
    "    tr_texts = texts_train\n",
    "    val_texts = np.array(val_ds[\"text\"])\n",
    "    use_split_indices = None\n",
    "\n",
    "test_texts = np.array(test_ds[\"text\"]) if test_ds is not None else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1df084bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized: train=(1804874, 100000), val=(97320, 100000), test=(97320, 100000)\n",
      "toxicity: val_auc=0.938 val_f1=0.625 val_acc=0.893 | test_auc=0.949 test_f1=0.543 test_acc=0.882\n",
      "severe_toxicity: val_auc=nan val_f1=0.000 val_acc=1.000 | test_auc=nan test_f1=0.000 test_acc=1.000\n",
      "obscene: val_auc=0.948 val_f1=0.380 val_acc=0.979 | test_auc=0.972 test_f1=0.320 test_acc=0.979\n",
      "threat: val_auc=0.965 val_f1=0.195 val_acc=0.976 | test_auc=0.965 test_f1=0.120 test_acc=0.975\n",
      "insult: val_auc=0.951 val_f1=0.620 val_acc=0.914 | test_auc=0.961 test_f1=0.523 test_acc=0.903\n",
      "identity_attack: val_auc=0.962 val_f1=0.346 val_acc=0.960 | test_auc=0.971 test_f1=0.217 test_acc=0.956\n",
      "sexual_explicit: val_auc=0.967 val_f1=0.319 val_acc=0.988 | test_auc=0.973 test_f1=0.245 test_acc=0.987\n"
     ]
    }
   ],
   "source": [
    "# Vectorize text once and reuse for all labels\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=100_000,\n",
    "    min_df=2,\n",
    "    stop_words=\"english\",\n",
    "    dtype=np.float32,\n",
    ")\n",
    "X_train = tfidf_vectorizer.fit_transform(tr_texts)\n",
    "X_val = tfidf_vectorizer.transform(val_texts)\n",
    "X_test = tfidf_vectorizer.transform(test_texts) if test_texts is not None else None\n",
    "\n",
    "\n",
    "def safe_auc(y_true, y_score):\n",
    "    y_true = np.asarray(y_true)\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        return float(\"nan\")\n",
    "    return roc_auc_score(y_true, y_score)\n",
    "\n",
    "\n",
    "# Train one model per label\n",
    "logreg_models = {}\n",
    "metrics_rows = []\n",
    "print(\n",
    "    f\"Vectorized: train={X_train.shape}, val={X_val.shape}\"\n",
    "    + (f\", test={X_test.shape}\" if X_test is not None else \"\")\n",
    ")\n",
    "\n",
    "for label in label_cols:\n",
    "    # Building binary targets (threshold at 0.5)\n",
    "    y_full_train = np.array(train_ds[label])\n",
    "    if use_split_indices is None:\n",
    "        y_train = (y_full_train >= 0.4).astype(int)\n",
    "        y_val = (np.array(val_ds[label]) >= 0.4).astype(int)\n",
    "    else:\n",
    "        idx_tr, idx_v = use_split_indices\n",
    "        y_train = (y_full_train[idx_tr] >= 0.4).astype(int)\n",
    "        y_val = (y_full_train[idx_v] >= 0.4).astype(int)\n",
    "\n",
    "    y_test = (\n",
    "        (np.array(test_ds[label]) >= 0.5).astype(int) if X_test is not None else None\n",
    "    )\n",
    "\n",
    "    # Logistic Regression model\n",
    "    clf = LogisticRegression(\n",
    "        solver=\"liblinear\",\n",
    "        class_weight=\"balanced\",\n",
    "        max_iter=1000,\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "    logreg_models[label] = clf\n",
    "\n",
    "    # Validation metrics\n",
    "    val_scores = clf.decision_function(X_val)\n",
    "    val_pred = (val_scores >= 0).astype(int)\n",
    "    val_auc = safe_auc(y_val, val_scores)\n",
    "    val_f1 = f1_score(y_val, val_pred)\n",
    "    val_acc = accuracy_score(y_val, val_pred)\n",
    "\n",
    "    msg = f\"{label}: val_auc={val_auc:.3f} val_f1={val_f1:.3f} val_acc={val_acc:.3f}\"\n",
    "\n",
    "    row = {\n",
    "        \"label\": label,\n",
    "        \"val_auc\": float(val_auc) if not np.isnan(val_auc) else np.nan,\n",
    "        \"val_f1\": float(val_f1),\n",
    "        \"val_acc\": float(val_acc),\n",
    "    }\n",
    "\n",
    "    # Test metrics\n",
    "    if X_test is not None:\n",
    "        test_scores = clf.decision_function(X_test)\n",
    "        test_pred = (test_scores >= 0).astype(int)\n",
    "        test_auc = safe_auc(y_test, test_scores)\n",
    "        test_f1 = f1_score(y_test, test_pred)\n",
    "        test_acc = accuracy_score(y_test, test_pred)\n",
    "        msg += (\n",
    "            f\" | test_auc={test_auc:.3f} test_f1={test_f1:.3f} test_acc={test_acc:.3f}\"\n",
    "        )\n",
    "\n",
    "        row.update(\n",
    "            {\n",
    "                \"test_auc\": float(test_auc) if not np.isnan(test_auc) else np.nan,\n",
    "                \"test_f1\": float(test_f1),\n",
    "                \"test_acc\": float(test_acc),\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        row.update(\n",
    "            {\n",
    "                \"test_auc\": np.nan,\n",
    "                \"test_f1\": np.nan,\n",
    "                \"test_acc\": np.nan,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    metrics_rows.append(row)\n",
    "    print(msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6413a8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "label",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "val_auc",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "val_f1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "val_acc",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "test_auc",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "test_f1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "test_acc",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "acebd6cc-16d8-46ba-ba55-0b3d91efd3b3",
       "rows": [
        [
         "0",
         "toxicity",
         "0.938",
         "0.625",
         "0.893",
         "0.949",
         "0.543",
         "0.882"
        ],
        [
         "1",
         "severe_toxicity",
         null,
         "0.0",
         "1.0",
         null,
         "0.0",
         "1.0"
        ],
        [
         "2",
         "obscene",
         "0.948",
         "0.38",
         "0.979",
         "0.972",
         "0.32",
         "0.979"
        ],
        [
         "3",
         "threat",
         "0.965",
         "0.195",
         "0.976",
         "0.965",
         "0.12",
         "0.975"
        ],
        [
         "4",
         "insult",
         "0.951",
         "0.62",
         "0.914",
         "0.961",
         "0.523",
         "0.903"
        ],
        [
         "5",
         "identity_attack",
         "0.962",
         "0.346",
         "0.96",
         "0.971",
         "0.217",
         "0.956"
        ],
        [
         "6",
         "sexual_explicit",
         "0.967",
         "0.319",
         "0.988",
         "0.973",
         "0.245",
         "0.987"
        ],
        [
         "7",
         "AVG",
         "0.955",
         "0.355",
         "0.958",
         "0.965",
         "0.281",
         "0.955"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 8
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>val_auc</th>\n",
       "      <th>val_f1</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>test_auc</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>test_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>toxicity</td>\n",
       "      <td>0.938</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.893</td>\n",
       "      <td>0.949</td>\n",
       "      <td>0.543</td>\n",
       "      <td>0.882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>severe_toxicity</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>obscene</td>\n",
       "      <td>0.948</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.972</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>threat</td>\n",
       "      <td>0.965</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.976</td>\n",
       "      <td>0.965</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>insult</td>\n",
       "      <td>0.951</td>\n",
       "      <td>0.620</td>\n",
       "      <td>0.914</td>\n",
       "      <td>0.961</td>\n",
       "      <td>0.523</td>\n",
       "      <td>0.903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>identity_attack</td>\n",
       "      <td>0.962</td>\n",
       "      <td>0.346</td>\n",
       "      <td>0.960</td>\n",
       "      <td>0.971</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sexual_explicit</td>\n",
       "      <td>0.967</td>\n",
       "      <td>0.319</td>\n",
       "      <td>0.988</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AVG</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.958</td>\n",
       "      <td>0.965</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             label  val_auc  val_f1  val_acc  test_auc  test_f1  test_acc\n",
       "0         toxicity    0.938   0.625    0.893     0.949    0.543     0.882\n",
       "1  severe_toxicity      NaN   0.000    1.000       NaN    0.000     1.000\n",
       "2          obscene    0.948   0.380    0.979     0.972    0.320     0.979\n",
       "3           threat    0.965   0.195    0.976     0.965    0.120     0.975\n",
       "4           insult    0.951   0.620    0.914     0.961    0.523     0.903\n",
       "5  identity_attack    0.962   0.346    0.960     0.971    0.217     0.956\n",
       "6  sexual_explicit    0.967   0.319    0.988     0.973    0.245     0.987\n",
       "7              AVG    0.955   0.355    0.958     0.965    0.281     0.955"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build metrics table\n",
    "metrics_df = pd.DataFrame(\n",
    "    metrics_rows,\n",
    "    columns=[\n",
    "        \"label\",\n",
    "        \"val_auc\",\n",
    "        \"val_f1\",\n",
    "        \"val_acc\",\n",
    "        \"test_auc\",\n",
    "        \"test_f1\",\n",
    "        \"test_acc\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Compute mean of numeric columns\n",
    "avg_vals = metrics_df.drop(columns=[\"label\"]).mean(numeric_only=True)\n",
    "avg_row = {**{\"label\": \"AVG\"}, **avg_vals.to_dict()}\n",
    "metrics_df = pd.concat([metrics_df, pd.DataFrame([avg_row])], ignore_index=True)\n",
    "\n",
    "# Round for readability\n",
    "for col in metrics_df.columns:\n",
    "    if col != \"label\":\n",
    "        metrics_df[col] = metrics_df[col].round(3)\n",
    "\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b6391d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models and vectorizer saved in the 'models' directory\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import os\n",
    " \n",
    "# Create a directory to save the models if it doesn't exist\n",
    "os.makedirs('inference/models', exist_ok=True)\n",
    "\n",
    "# Save the TF-IDF vectorizer\n",
    "joblib.dump(tfidf_vectorizer, 'inference/models/tfidf_vectorizer.joblib')\n",
    "\n",
    "# Save each logistic regression model\n",
    "for label, model in logreg_models.items():\n",
    "    model_filename = f'inference/models/logreg_{label}.joblib'\n",
    "    joblib.dump(model, model_filename)\n",
    "\n",
    "print(\"Models and vectorizer saved in the 'models' directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e4f0d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark (CPU) on test set:\n",
      " - samples: 97320\n",
      " - total_inference_seconds_all_labels: 0.070732\n",
      " - throughput_samples_per_sec_all_labels: 1375895.81\n",
      " - avg_per_sample_latency_ms_all_labels: 0.000727\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "label",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "test_infer_seconds",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "per_sample_ms",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "4c7429d2-84ac-41a2-b595-c3d1819198d9",
       "rows": [
        [
         "0",
         "toxicity",
         "0.011617",
         "0.000119"
        ],
        [
         "1",
         "severe_toxicity",
         "0.011108",
         "0.000114"
        ],
        [
         "2",
         "obscene",
         "0.010488",
         "0.000108"
        ],
        [
         "3",
         "threat",
         "0.009916",
         "0.000102"
        ],
        [
         "4",
         "insult",
         "0.009777",
         "0.0001"
        ],
        [
         "5",
         "identity_attack",
         "0.009187",
         "9.4e-05"
        ],
        [
         "6",
         "sexual_explicit",
         "0.008437",
         "8.7e-05"
        ],
        [
         "7",
         "AVG",
         "0.010076",
         "0.000104"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 8
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>test_infer_seconds</th>\n",
       "      <th>per_sample_ms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>toxicity</td>\n",
       "      <td>0.011617</td>\n",
       "      <td>0.000119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>severe_toxicity</td>\n",
       "      <td>0.011108</td>\n",
       "      <td>0.000114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>obscene</td>\n",
       "      <td>0.010488</td>\n",
       "      <td>0.000108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>threat</td>\n",
       "      <td>0.009916</td>\n",
       "      <td>0.000102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>insult</td>\n",
       "      <td>0.009777</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>identity_attack</td>\n",
       "      <td>0.009187</td>\n",
       "      <td>0.000094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sexual_explicit</td>\n",
       "      <td>0.008437</td>\n",
       "      <td>0.000087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AVG</td>\n",
       "      <td>0.010076</td>\n",
       "      <td>0.000104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             label  test_infer_seconds  per_sample_ms\n",
       "0         toxicity            0.011617       0.000119\n",
       "1  severe_toxicity            0.011108       0.000114\n",
       "2          obscene            0.010488       0.000108\n",
       "3           threat            0.009916       0.000102\n",
       "4           insult            0.009777       0.000100\n",
       "5  identity_attack            0.009187       0.000094\n",
       "6  sexual_explicit            0.008437       0.000087\n",
       "7              AVG            0.010076       0.000104"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CPU inference benchmarking on test set (LogReg one-vs-rest)\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Optional warm-up to avoid one-time overheads\n",
    "_ = logreg_models[label_cols[0]].decision_function(X_test)\n",
    "\n",
    "per_label_timings = []\n",
    "t_total_start = time.perf_counter()\n",
    "for label in label_cols:\n",
    "    clf = logreg_models[label]\n",
    "    t0 = time.perf_counter()\n",
    "    _ = clf.decision_function(X_test)\n",
    "    t1 = time.perf_counter()\n",
    "    per_label_timings.append({\n",
    "        \"label\": label,\n",
    "        \"test_infer_seconds\": t1 - t0,\n",
    "    })\n",
    "t_total_end = time.perf_counter()\n",
    "\n",
    "total_seconds = t_total_end - t_total_start\n",
    "n_samples = X_test.shape[0]\n",
    "\n",
    "# Build timing table\n",
    "time_metrics_df = pd.DataFrame(per_label_timings)\n",
    "time_metrics_df[\"per_sample_ms\"] = (time_metrics_df[\"test_infer_seconds\"] / n_samples) * 1000.0\n",
    "\n",
    "# Append AVG row\n",
    "avg_vals = time_metrics_df.drop(columns=[\"label\"]).mean(numeric_only=True)\n",
    "avg_row = {**{\"label\": \"AVG\"}, **avg_vals.to_dict()}\n",
    "time_metrics_df = pd.concat([time_metrics_df, pd.DataFrame([avg_row])], ignore_index=True)\n",
    "\n",
    "# Round for readability\n",
    "time_metrics_df[\"test_infer_seconds\"] = time_metrics_df[\"test_infer_seconds\"].round(6)\n",
    "time_metrics_df[\"per_sample_ms\"] = time_metrics_df[\"per_sample_ms\"].round(6)\n",
    "\n",
    "# Summary\n",
    "throughput = n_samples / total_seconds if total_seconds > 0 else float(\"inf\")\n",
    "per_sample_ms_all = (total_seconds / n_samples) * 1000.0\n",
    "\n",
    "print(\"Benchmark (CPU) on test set:\")\n",
    "print(f\" - samples: {n_samples}\")\n",
    "print(f\" - total_inference_seconds_all_labels: {total_seconds:.6f}\")\n",
    "print(f\" - throughput_samples_per_sec_all_labels: {throughput:.2f}\")\n",
    "print(f\" - avg_per_sample_latency_ms_all_labels: {per_sample_ms_all:.6f}\")\n",
    "\n",
    "time_metrics_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".pyenv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
